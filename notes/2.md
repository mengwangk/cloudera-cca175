CCA175 -2 

https://www.udemy.com/cca-175-spark-and-hadoop-developer-certification-scala/learn/v4/content
https://www.youtube.com/watch?v=GFSqn90ArE0&t=172s&index=2&list=PLRLUm7no962j8cf-mpXjrQqusWvw-gIJx
http://arun-teaches-u-tech.blogspot.my/p/certification-preparation-plan.html
https://www.coursera.org/learn/machine-learning/home/week/1


# du -sh /XX
# hadoop fs -ls /public
# hadoop fs -du -s -h /public

# export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")


Etc/hadoop
- Core-site.xml
- hdfs-site.xml

# hadoop fs -put local dest
# hadoop fs -get remote local
# hadoop fs -du -s -h
# hdfs fsck /uXXX -files -blocks -locations

hadoop checknative -a

- Yarn-site.xml
- Spark-env.xml

Hive setting up Derby
hive --hiveconf hive.root.logger=DEBUG,console

https://cwiki.apache.org/confluence/display/Hive/HiveDerbyServerMode#HiveDerbyServerMode-StartingDerby
https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool

schematool -dbType derby -initSchema


> sqoop list-databases \
--connect "jdbc:mysql://localhost:3306" --username retail_dba --password cloudera

> sqoop list-tables \
--connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera

> sqoop eval --query "select * from order_items limit 10" --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera

> sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera \
--table order_items --target-dir /user/cloudera/order_items --direct
 
> sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items --target-dir /user/cloudera/order_items --compression-codec=snappy --as-avrodatafile --compress --num-mappers 1

sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items_nopk --target-dir /user/cloudera/order_items --compression-codec=snappy --as-avrodatafile --compress --num-mappers  1
--delete-target-dir
--append

// No primary, but for performance use indexed columns
// Values in the filed should be sparse
// also often it should be sequence generated or evenly incremented
// Should not have NULL values
sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items_nopk --target-dir /user/cloudera/order_items --compression-codec=snappy --as-avrodatafile --compress --num-mappers  2 --split-by order_item_order_id

sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table orders --target-dir /user/cloudera/orders  --compression-codec=snappy --as-avrodatafile --compress --num-mappers  2 --split-by order_status -Dorg.apache.sqoop.splitter.allow_text_splitter=true

* By default sqoop import fails if the –target-dir or sub directory with table name under –warehouse-dir exists
* –hive-overwrite will delete and recreate the directory
* –append will create new files under existing directory

sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table orders --target-dir /user/cloudera/orders  --compression-codec=org.apache.hadoop.io.compress.SnappyCodec --as-avrodatafile --compress --num-mappers  2  --delete-target-dir

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
--num-mappers 1 \
--delete-target-dir

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
--num-mappers 1 \
--append


Things to remember for Sqoop split by:
* column should be indexed, otherwise performance will be significantly poor for even medium size tables
* values in the field should be sparse
* also often it should be sequence generated or evenly incremented
* it should not have null values
* data can be split on non numeric fields such as tax, but we need to use -Dorg.apache.sqoop.splitter.allow_text_splitter=true

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table order_items_nopk \
--warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
--split-by order_item_order_id

#Splitting on text field
sqoop import \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
--split-by order_status


sqoop mappers

--autoreset-to-one-mapper  if there is no primary key or no --split-by

sqoop file format
- text file, sequence file, avro and parquet

--as-textfile, as-sequencefile, as-avrodatafile, as-parquetfile

sqoop compression
--compress
--compress-codec=snappy|gzip|

default to gzip

Snappy installation
http://hadooptutorial.info/snappy-compressiondecompression-tool/
http://google.github.io/snappy/
https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
  --num-mappers 2 \
  --as-textfile \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.GzipCodec

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
  --num-mappers 2 \
  --as-textfile \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec

sqoop boundary query
--boundary-query 'select min(order_item_id), max(order_item_id) from order_items where  order_item_id > 9999'
--boundary-query 'select 1, 10000'


sqoop column and query
--table order_items
--columnns order_item_order_id,order_item_id,order_item_subtotal

or

--query 'select o.*, sum(oi.order_item_subtotal) order_revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id  and \$CONDITIONS group by o.order_id, o.order_date, o.order_customer_id, o.order_status'
--split-by order_id (if num-mappers > 1)
--target-dir /user/cloudera/order_revenue_summary


sqoop - delimiter and handling nulls

sqoop import --connect "jdbc:mysql://localhost:3306/hr_db" --username hr_dba --password cloudera --table employees --target-dir /user/cloudera/employees  --compression-codec=org.apache.hadoop.io.compress.SnappyCodec  --compress --num-mappers  2  --delete-target-dir

sqoop import --connect "jdbc:mysql://localhost:3306/hr_db" --username hr_dba --password cloudera --table employees --target-dir /user/cloudera/employees --num-mappers  2  --delete-target-dir --null-string 'nothing'

--null-non-string 

--fields-terminated-by '\t'
--lines-terminated-by ':'
--fields-terminated-by '\000' (null character)

--enclosed-by <char>	Sets a required field enclosing character
--escaped-by <char>	Sets the escape character
--fields-terminated-by <char>	Sets the field separator character
--lines-terminated-by <char>	Sets the end-of-line character
--mysql-delimiters	Uses MySQL’s default delimiter set: fields: , lines: \n escaped-by: \ optionally-enclosed-by: '
--optionally-enclosed-by <char>	Sets a field enclosing character

hdfs dfs -cat /user/cloudera/employees/part-m-00000


sqoop - incremental import

--query "select * from orders where \$CONDITIONS and order_date like '2013-%'
--num-mappers 2
--split-by order_id
--append

sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --target-dir /user/cloudera/orders  --compression-codec=org.apache.hadoop.io.compress.SnappyCodec --as-avrodatafile --compress  --query "select * from orders where \$CONDITIONS and order_date like '2013-%'" --num-mappers 2 --split-by order_id --append

--table orders
--where "order_date like '2014-02%'
--append

or

--table orders
--check-column  order_date
--incremental append
--last-value '2014-02-28'


sqoop hive

Simple hive import - append to existing table
sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items --hive-import --hive-database cca175 --hive-table order_items --num-mappers 2

sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table orders --hive-import --hive-database cca175 --hive-table orders --num-mappers 2

hive> describe formatted order_items;


Overwrite
sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items --hive-import --hive-database cca175 --hive-table order_items --num-mappers 2 --hive-overwrite

Failed if table already exists - create staging table under /user/mengwangk/order_items
sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera --table order_items --hive-import --hive-database cca175 --hive-table order_items --num-mappers 2 --create-hive-table


sqoop import all tables

sqoop import-all-tables --connect "jdbc:mysql://localhost:3306/retail_db" --username retail_dba --password cloudera  --warehouse-dir /user/cloudera/retail_db --autoreset-to-one-mapper


sqoop export

mysql
create table daily_revenue as select order_date, sum(order_item_subtotal) daily_revenue from orders join order_items on order_id=order_item_order_id where order_date like '2013-07%' group by order_date 


hdfs://localhost:9000/user/hive/warehouse/cca175.db/daily_revenue

sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue --num-mappers 4

Column mapping
sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue_demo --num-mappers 4 --columns order_date,revenue

create table daily_revenue_demo(
	revenue float,
	order_date varchar(30),
	description varchar(200)
);

Stored Proc
--call <stored-proc-name>


sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue_pk --num-mappers 4 --columns order_date,revenue --update-mode allowInsert

Update
create table daily_revenue_pk(
	revenue float,
	order_date varchar(30) primary key,
	description varchar(200)
);

In hive, append additional records

insert into daily_revenue select order_date, sum(order_item_subtotal) daily_revenue from orders join order_items on order_id=order_item_order_id where order_date like '2013-08%' group by order_date 

sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue --num-mappers 4 --columns order_date,revenue --update-mode allowinsert --update-key order-date


Stage Tables

insert into daily_revenue select order_date, sum(order_item_subtotal) daily_revenue from orders join order_items on order_id=order_item_order_id where order_date > '2013-08' group by order_date 

 insert into daily_revenue values('2014-07-01 00:00:00.0', 0);

Failed to insert
sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue --num-mappers 1

create table daily_revenue_staging(
	order_date varchar(30) primary key,
	revenue float
);

sqoop  export  --connect "jdbc:mysql://localhost:3306/retail_export" --username retail_dba --password cloudera  --export-dir /user/hive/warehouse/cca175.db/daily_revenue --input-fields-terminated-by "\001" --table daily_revenue --num-mappers 1 --staging-table daily_revenue_staging --clear-staging-table


Machine Learning

Week 1
Supervised learning
- Regression
- Classification 
- How to deal with infinite number of attributes or features?
Unsupervised learning
- Clustering
- Organise computing clusters, social network analysis, market segmentation, astronomical data analysis
- Cocktail Party Problem
    - Audio processing..
    - [W,s,v]=svd((repmat….
    - Octave or Matlab

Regression
- real value output
Classification
- discrete value
- 
univariate regression - 1 variable

Cost function = h(x) = 1+2x
... y = 1 + x










